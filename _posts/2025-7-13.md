---
layout: post
title: TExpr dev-log 1
---
Currently, texpr is targeting a more low-level representation than what was originally planned.

The TE representation. N represents a concrete natural. $i the symbolic name of indices, $v the symbolic name of tensors.
```
tstmt s = par N $i [s]
        | seq N $i [s]
        | bind $v iexpr texpr
        | new $v N
        | anchor texpr

texpr e = view $v iexpr
        | op [e]

iexpr i = I $i  -- the iteration index
        | C Z   -- constant integer
        | Neg i
        | Add i i
        | Mul i i
        | Div i i -- floor division
```

whereas before it was:
```
texpr e = compute N (λ $i . e)
        | reduce N (λ $i . e)
        | let $v e1 e2  -- let v = e1 in e2
        | index $v iexpr
        | Elemwise [e]
```

I took the lower level route because the higher level route is already well explored by works like tiramisu, tvm, triton, tensor comprehensions, futhark to name a few that I'm aware of. However, there are some limitations with these works that I think fail to solve the problems of performance engineers at ML companies.
1. I need a tool that is stand-alone and is entirely ahead of time compiled. It should generate CUDA or equivalent program that can be easily integrated into a C++/Rust codebase that uses a mish-mash of other libraries like libtorch and TensorRT.
2. I need a tool that simplifies CUDA programming, not one that tries to replace CUDA programming or performance engineers. Current approaches that do so are too high-level and thus are restricted in terms of the programs that can be written and the kind of control over the optimizations available. 
- all of the tools mentioned either operate at a tile-level where shared memory is not exposed as an abstraction, or at an even higher level where it is not clear what is a single kernel, what parts are in on-chip memory, in registers, etc.
- All of the tools, either using polyhedral techniques or traditional compiler passes can do some amount of optimizations. Automatic parallelization, memory promotion, layout selection, pipelining, using tensor cores. But Nvidia has gone crazy and added new features in ptx that I think will be extremely difficult to schedule from such high-level representations. Some of these features that I think are useful are: 
  - CTA clusters. Another level of parallelism, so now we have [grids, clusters, warps, threads]
  - mbarrier. Using arrive/wait semantics on tokens in shared memory to synchronize arbitrary set of threads
  - more async instructions. cp.async, st.async are asynchronous transactions between global and shared. (There's also TMA, and tcgen, which are bulk tensor copying async instructions and async mma instructions on blackwell, but haven't looked too much into those)
  - special red/atomic instructions. 
In all of these cases, the general patterns is adding more asynchronous instructions, and adding more special pipelines to hide latency. In blackwell I'm aware that matrix multiplication is almost treated as it's own unit that's mostly asynchronous and divorced from the regular simt pipelines, so only a single thread need to issue the instruction. In hopper there's a new optimization pattern called warp specialization enabled by a feature that adjusts the number of registers per warp, where warps takes on different roles (producer/consumer). The difficult part comes in scheduling, in keeping all the pipeline saturated, the difficult part comes in organizing the combinatorial nature of optimizations, in balancing optimizations with each other, but also in code reuse and understanding. CUDA is not enough, it is too low level and C++ abstractions are not sufficient and thus leads to significant code duplication. I think this is both a programming language design problem as well as an optimization problem. What can be automated? What can be abstracted?

Right now I think coalesced memory access and layout selection can be automated, basic instruction reordering and asynchonous execution can be automatically found. Kernel fusion can be automated. Triton already does this (except for kernel fusion), but does not provide lower level escape hatches for engineers to perform other optimizations. Trition also only expresses one level of parallelism at the block level, whereas I think it would be more valuable to express arbitrary levels of parallelism both for compositionality as well as capturing the hierarchical nature of processors.
I'm aware that MLIR to a certain degree achieves the goals listed above, although through a different interface. Users are expected to, given an input program, write a transformation IR similar to tvm that applies the existing passes. The advantages of this is portability as the transformation IR can be tuned per accelerator, while the original program stays the same. Performance is white-box since the transformed IR can be inspected and expresses low-level details after lowering. However, as far as I know, the pass selection is a bit limited, and despite claiming to support polyhedral optimization techniques, it is mostly used in analysis for traditional loop transformations like fusion and tiling. I don't know if this is the right interface, but MLIR is a large dependency. (Maybe one can package it using some kind of PL abstraction? At least support composition of passes like a library?)

I have some inkling of how to handle expressing combinatorial optimizations through congruence relations as done in equality saturation (which is not a dialect in MLIR, afaik). To do so we need a different expression-like form of the IR with no statements. 

Let I be the type of index expressions and E the type of tensor expressions.
```
tgraph = par (shape: N) (level: N) : I
       | seq (shape: N) (level: N) : I
       | view (g : E) (i : I) : E
       | bind (e0 : E) (i : I) (e1 : E) : E
       | op ([e] : [E]) : [E]
       | new N : E
       | param $s : E
       | fix ([e] : [E]) $i : [E]
```
I think there's an equivalence between this representation and the statement representation, modulo the execution order. Here everything is an expression and consequently has a type. view and bind are reads and writes, except bind is linear and consumes its argument (the written tensor). 
Here's matmul between the two forms:
```
par M i {
    par N j {
        seq K k {
            C[i * N + j] += A[i * K + k] * B[k * N + j]
        }
    }
}
===
i = par M 0 : I { $i }
j = par N 1 : I { $j }
k = seq K 2 : I { $k }
a0 = view A (i * K + k) : E { idx={$i, $k}, shape=[1] }
b0 = view B (k * N + j) : E { idx={$k, $j}, shape=[1] }
c0 = view C (i * N + j) : E { idx={$i, $j}, shape=[1] }
c' = a0 * b0 + c0 : E { idx={$i, $j, $k}, shape=[1] }
c'' = fix c' $k : E { idx={$i, $j}, shape=[1] }
C = bind C (i * N + j) c'' : E { idx = {}, shape=[M * N]}
```
The type I, that of index expressions includes its index set, that is the set of bounded indices introduced by `par` and `seq`. The type E, that of tensor expressions includes the index set and the current realized shape. However, I do not have a proof of general equivalence, and I think it is not entirely trivial to convert the graphical form to the statement form.
Here's the tiled version:
```
par (N/Tn, M/Tm) |it, jt| {
    let at [Tn, Tk]
    let bt [Tk, Tm]
    let ct [Tn, Tm] = 0;
    seq (K/Tk) |k| {
        par (Tn, Tk) |i, j| {
            at[i, j] = a[i + it * Tn, j + k * Tk];
        }
        par (Tk, Tm) |i, j| {
            bt[i, j] = b[i + k * Tk, j + jt * Tm];
        }

        ct += inner_mma(at, bt);
    }
    par (Tn, Tm) |i, j| {
        c[i + it * Tn, j + jt * Tm] = ct[i, j];
    }
}
===
it = par (N / Tn) : I { $it }
jt = par (M / Tm) : I { $it }
i = par Tn : I { $i }
j = par Tm : I { $j }
k = par Tk : I { $k }
k' = seq (K / Tk) { $k' }
at' = view A (it * Tm + i, k' * Tk + k) : E { idx={$it, $i, $k, $k' }, shape=[1]}
at = new (Tm, Tk) : E { idx = {}, shape=[Tm, Tk] }
at = bind at (i, k) at' : E { idx = {$it, $k'}, shape=[Tm, Tk] }

bt' = view B (k' * Tk + k, jt * Tn + j) : E { idx = {$k', $k, $jt, $j}}
bt = new (Tk, Tn) : E { idx = {}, shape=[Tk, Tn]}
bt = bind bt (k, j) bt' : E { idx = {$k', $jt}, shape = [Tk, Tn]}

ct = inner_mma(at, bt) : E { idx = {$k', $jt, $it}, shape = [Tm, Tn] }
c' = view ct (i, j) : E { idx = {$k', $it, $i, $jt, $j}, shape=[1] }
c' = fix c' $k' : E { idx = { $it, $i, $jt, $j }, shape=[1]}
C = bind C (it * Tm + i, jt * Tn + j) c' : E { idx = {}, shape = [M, N] }
```
